{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6986bee2",
   "metadata": {},
   "source": [
    "# Code for Structured Perceptron (not in book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "users = re.compile('@[^ ]+')\n",
    "numbers = re.compile('[0-9]')\n",
    "urls = re.compile(\"(https?:\\/\\/)?(?:www\\.|(?!www))?[^\\s\\.]+\\.[^\\s]{2,}|(www)?\\.[^\\s]+\\.[^\\s]{2,}\")\n",
    "\n",
    "#++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "class StructuredPerceptron(object):\n",
    "    \"\"\"\n",
    "    implements a structured perceptron as described in Collins 2002,\n",
    "    with updates from https://explosion.ai/blog/part-of-speech-pos-tagger-in-python\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        initialize model parameters\n",
    "        \"\"\"\n",
    "        self.tags = set()\n",
    "        # argmax Y = wX + b\n",
    "        # features = {f1, f2, f3, f4}\n",
    "        # P(ADJ|f1)?\n",
    "        # feature_weights = { f1: {'ADJ': 3.14, 'VERB': 2.65, 'NOUN':...},\n",
    "        #                     f2: {'ADJ': 1.16, 'VERB': 7.15, 'NOUN':...}, }\n",
    "        self.feature_weights = defaultdict(lambda: defaultdict(float)) #feature_name -> tags -> weight\n",
    "        self.weight_totals = defaultdict(lambda: defaultdict(float)) #feature_name -> tags -> weight\n",
    "        self.timestamps = defaultdict(lambda: defaultdict(float)) #feature_name -> tags -> weight\n",
    "\n",
    "        self.tag_dict = defaultdict(set) #word -> {tags}\n",
    "\n",
    "        self.START = \"__START__\"\n",
    "        self.END = \"__END__\"\n",
    "        \n",
    "        \n",
    "    def normalize(self, word):\n",
    "        \"\"\"\n",
    "        lowercase word, and replace numbers, user names, and URLs\n",
    "        \"\"\"\n",
    "        return re.sub(urls, 'URL', re.sub(users, '@USER', re.sub(numbers, '0', word.strip().lower())))\n",
    "\n",
    "    \n",
    "    def evaluate(self, data_instances, method='greedy'):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for (words, tags) in data_instances:\n",
    "            preds = self.predict(words, method=method)\n",
    "            matches = sum(map(lambda x: int(x[0]==x[1]), zip(preds, tags)))\n",
    "            correct += matches\n",
    "            total += len(tags)\n",
    "        return correct/total\n",
    "        \n",
    "    \n",
    "    def fit(self, file_name, \n",
    "            dev_file=None, \n",
    "            iterations=10, \n",
    "            learning_rate=0.25, \n",
    "            inference='greedy', \n",
    "            verbose=False):\n",
    "        \"\"\"\n",
    "        read in a CoNLL-format file, extract features to train weight vector\n",
    "        \"\"\"        \n",
    "        # initialize tag dictionary for each word and get tag set\n",
    "        instances = [(words, tags) for (words, tags) in self.read_conll_file(file_name)]\n",
    "        for (words, tags) in instances:\n",
    "            self.tags.update(set(tags))\n",
    "\n",
    "            for word, tag in zip(words, tags):\n",
    "                self.tag_dict[self.normalize(word)].add(tag)\n",
    "        \n",
    "        if dev_file:\n",
    "            dev_instances = [(words, tags) for (words, tags) in self.read_conll_file(dev_file)]\n",
    "            \n",
    "        # iterate over data\n",
    "        for iteration in range(1, iterations+1):\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            if verbose:\n",
    "                print('Iteration {}'.format(iteration+1), file=sys.stderr, flush=True)\n",
    "                print(\"*\" * 15, file=sys.stderr, flush=True)\n",
    "\n",
    "            random.shuffle(instances)\n",
    "            for i, (words, tags) in enumerate(instances):\n",
    "                if i > 0:\n",
    "                    if i%1000==0:\n",
    "                        print('%s'%i, file=sys.stderr, flush=True)\n",
    "                    elif i%20==0:\n",
    "                        print('.', file=sys.stderr, flush=True, end='')\n",
    "\n",
    "                # get prediction\n",
    "                prediction = self.predict(words, method=inference)\n",
    "\n",
    "                # derive global features\n",
    "                global_gold_features, global_prediction_features = self.get_global_features(words, prediction, tags)\n",
    "                                    \n",
    "                # update weight vector:\n",
    "                # 1. move closer to true tag\n",
    "                for tag, fids in global_gold_features.items():\n",
    "                    for fid, count in fids.items():\n",
    "                        nr_iters_at_this_weight = iteration - self.timestamps[fid][tag]\n",
    "                        self.weight_totals[fid][tag] += nr_iters_at_this_weight * self.feature_weights[fid][tag]\n",
    "                        self.timestamps[fid][tag] = iteration\n",
    "                        self.feature_weights[fid][tag] += learning_rate * count\n",
    "\n",
    "                # 2. move further from wrong tag\n",
    "                for tag, fids in global_prediction_features.items():\n",
    "                    for fid, count in fids.items():\n",
    "                        nr_iters_at_this_weight = iteration - self.timestamps[fid][tag]\n",
    "                        self.weight_totals[fid][tag] += nr_iters_at_this_weight * self.feature_weights[fid][tag]\n",
    "                        self.timestamps[fid][tag] = iteration\n",
    "                        self.feature_weights[fid][tag] -= learning_rate * count\n",
    "                        \n",
    "                # compute training accuracy for this iteration\n",
    "                correct += sum([int(predicted_tag == true_tag) for predicted_tag, true_tag in zip(prediction, tags)])\n",
    "                total += len(tags)\n",
    "\n",
    "                # output examples\n",
    "                if verbose and i%1000==0:\n",
    "                    print(\"current word accuracy:{:.2f}\".format(correct/total))\n",
    "                    print(list(zip(words, \n",
    "                                   [self.normalize(word) for word in words], \n",
    "                                   tags, \n",
    "                                   prediction)), file=sys.stderr, flush=True)\n",
    "            \n",
    "            print('\\t{} features'.format(len(self.feature_weights)), file=sys.stderr, flush=True)\n",
    "            print('\\tTraining accuracy: {:.2f}\\n'.format(correct/total), file=sys.stderr, flush=True)\n",
    "            if dev_file:\n",
    "                print('\\tDevelopment accuracy: {:.2f}\\n'.format(self.evaluate(dev_instances, method=inference)), file=sys.stderr, flush=True)\n",
    "         \n",
    "        # average weights\n",
    "        for feature, tags in self.feature_weights.items():\n",
    "            for tag in tags:\n",
    "                total = self.weight_totals[feature][tag]\n",
    "                total += (iterations - self.timestamps[feature][tag]) * self.feature_weights[feature][tag]\n",
    "                averaged = round(total / float(iterations), 3)\n",
    "                self.feature_weights[feature][tag] = averaged\n",
    "\n",
    "\n",
    "    def get_features(self, word, previous_tag2, previous_tag, words, i):\n",
    "        \"\"\"\n",
    "        get all features that can be derived from the word and previous tags\n",
    "        \"\"\"\n",
    "        prefix = word[:3]\n",
    "        suffix = word[-3:]\n",
    "\n",
    "        features = {\n",
    "                    'PREFIX={}'.format(prefix),\n",
    "                    'SUFFIX={}'.format(suffix),\n",
    "                    'LEN<=3={}'.format(len(word)<=3),\n",
    "                    'FIRST_LETTER={}'.format(word[0]),\n",
    "                    'WORD={}'.format(word),\n",
    "                    'NORM_WORD={}'.format(words[i]),\n",
    "                    'PREV_WORD={}'.format(words[i-1]),\n",
    "                    'PREV_WORD_PREFIX={}'.format(words[i-1][:3]),\n",
    "                    'PREV_WORD_SUFFIX={}'.format(words[i-1][-3:]),\n",
    "                    'PREV_WORD+WORD={}+{}'.format(words[i-1], words[i]),\n",
    "                    'NEXT_WORD={}'.format(words[i+1]),\n",
    "                    'NEXT_WORD_PREFIX={}'.format(words[i+1][:3]),\n",
    "                    'NEXT_WORD_SUFFIX={}'.format(words[i+1][-3:]),\n",
    "                    'WORD+NEXT_WORD={}'.format(word, words[i+1]),\n",
    "                    'NEXT_2WORDS={}+{}'.format(words[i+1], words[i+2]),\n",
    "                    'PREV_TAG={}'.format(previous_tag),                 # previous tag\n",
    "                    'PREV_TAG2={}'.format(previous_tag2),                 # two-previous tag\n",
    "                    'PREV_TAG_BIGRAM={}+{}'.format(previous_tag2, previous_tag),  # tag bigram\n",
    "                    'PREV_TAG+WORD={}+{}'.format(previous_tag, word),            # word-tag combination\n",
    "                    'PREV_TAG+PREFIX={}_{}'.format(previous_tag, prefix),        # prefix and tag\n",
    "                    'PREV_TAG+SUFFIX={}_{}'.format(previous_tag, suffix),        # suffix and tag\n",
    "                    'WORD+TAG_BIGRAM={}+{}+{}'.format(word, previous_tag2, previous_tag),\n",
    "                    'SUFFIX+2TAGS={}+{}+{}'.format(suffix, previous_tag2, previous_tag),\n",
    "                    'PREFIX+2TAGS={}+{}+{}'.format(prefix, previous_tag2, previous_tag),\n",
    "                    'BIAS'\n",
    "            }\n",
    "        return features\n",
    "    \n",
    "    \n",
    "    def get_global_features(self, words, predicted_tags, true_tags):\n",
    "        '''\n",
    "        sum up local features\n",
    "        '''\n",
    "        context = [self.START] + [self.normalize(word) for word in words] + [self.END, self.END]\n",
    "\n",
    "        global_gold_features = defaultdict(lambda: Counter())\n",
    "        global_prediction_features = defaultdict(lambda: Counter())\n",
    "\n",
    "        prev_predicted_tag = self.START\n",
    "        prev_predicted_tag2 = self.START\n",
    "        \n",
    "        for j, (word, predicted_tag, true_tag) in enumerate(zip(words, predicted_tags, true_tags)):\n",
    "            # get the predicted features. NB: use j+1, since context is longer than words\n",
    "            prediction_features = self.get_features(word, prev_predicted_tag2, prev_predicted_tag, context, j+1)\n",
    "\n",
    "            # update feature correlation with true and predicted tag\n",
    "            global_prediction_features[predicted_tag].update(prediction_features)\n",
    "            global_gold_features[true_tag].update(prediction_features)\n",
    "\n",
    "            prev_predicted_tag2 = prev_predicted_tag\n",
    "            prev_predicted_tag = predicted_tag\n",
    "\n",
    "        return global_gold_features, global_prediction_features\n",
    "            \n",
    "    \n",
    "    def get_scores(self, features):\n",
    "        \"\"\"\n",
    "        predict scores for each tag given features\n",
    "        \"\"\"\n",
    "        scores = defaultdict(float)\n",
    "        \n",
    "        # add up the scores for each tag\n",
    "        for feature in features:\n",
    "            if feature not in self.feature_weights:\n",
    "                continue\n",
    "            weights = self.feature_weights[feature]\n",
    "            for tag, weight in weights.items():\n",
    "                scores[tag] += weight\n",
    "\n",
    "        # return tag scores\n",
    "        if not scores:\n",
    "            # if there are no scores (e.g., first iteration),\n",
    "            # simply return the first tag with score 1\n",
    "            scores[list(self.tags)[0]] = 1\n",
    "        \n",
    "        return scores\n",
    "\n",
    "\n",
    "    def predict(self, words, method='greedy'):\n",
    "        '''\n",
    "        predict tags using one of two methods\n",
    "        '''\n",
    "        if method == 'greedy':\n",
    "            return self.predict_greedy(words)\n",
    "        elif method == 'viterbi':\n",
    "            return self.predict_viterbi(words)\n",
    "\n",
    "\n",
    "    def predict_viterbi(self, words):\n",
    "        '''\n",
    "        predict using Viterbi decoding\n",
    "        '''\n",
    "        context = [self.START] + [self.normalize(word) for word in words] + [self.END, self.END]\n",
    "\n",
    "        N = len(words)\n",
    "        M = len(self.tags) #number of tags\n",
    "        tags = sorted(self.tags)\n",
    "\n",
    "        # create trellis of size M (number of tags) x N (sentence length)\n",
    "        Q = np.ones((M, N)) * float('-Inf')\n",
    "        backpointers = np.ones((M, N), dtype=np.int16) * -1 #backpointers\n",
    "\n",
    "        # initialize probs for tags j at position 1 (first word)\n",
    "        features = self.get_features(words[0], self.START, self.START, context, 1)\n",
    "        scores = self.get_scores(features)\n",
    "        allowed_initial_tags = self.tag_dict[context[1]]\n",
    "\n",
    "        for j in range(M):\n",
    "            if not allowed_initial_tags or tags[j] in allowed_initial_tags:\n",
    "                Q[j,0] = scores[tags[j]]\n",
    "\n",
    "        # filling the lattice, for every position and every tag find viterbi score Q\n",
    "        for i in range(1, N):\n",
    "            allowed_tags = self.tag_dict[context[i+1]]\n",
    "\n",
    "            # for every previous tag\n",
    "            for j in range(M):\n",
    "                best_score = 0.0#float('-Inf')\n",
    "                prev_tag = tags[j]\n",
    "\n",
    "                # skip impossible tags\n",
    "                allowed_previous_tags = self.tag_dict[context[i]]\n",
    "                if allowed_previous_tags and prev_tag not in allowed_previous_tags:\n",
    "                    continue\n",
    "\n",
    "                best_before = Q[j,i-1] # score of previous tag\n",
    "\n",
    "                # for every possible pre-previous tag\n",
    "                for k in range(M):\n",
    "                    if i == 1:\n",
    "                        prev2_tag = self.START\n",
    "                    else:\n",
    "                        prev2_tag = tags[k]\n",
    "                        # skip impossible tags\n",
    "                        allowed_previous2_tags = self.tag_dict[context[i-1]]\n",
    "                        if allowed_previous2_tags and prev2_tag not in allowed_previous2_tags:\n",
    "                            continue\n",
    "\n",
    "                    # get features of word i with the two previous tags\n",
    "                    features = self.get_features(words[i], prev2_tag, prev_tag, context, i+1)\n",
    "                    scores = self.get_scores(features)\n",
    "\n",
    "                    # update best score\n",
    "                    for t in range(M):\n",
    "                        tag = tags[t]\n",
    "                        # if word is unknown, use all tags, otherwise allowed ones\n",
    "                        if not allowed_tags or tag in allowed_tags:\n",
    "                            tag_score = best_before + scores[tag]\n",
    "\n",
    "                            if tag_score > best_score:\n",
    "                                Q[t,i] = tag_score\n",
    "                                best_score = tag_score\n",
    "                                backpointers[t,i] = j\n",
    "\n",
    "        # final best\n",
    "        best_id = Q[:,-1].argmax()\n",
    "\n",
    "        # print best tags in reverse order\n",
    "        predtags = [tags[best_id]]\n",
    "\n",
    "        for i in range(N-1,0,-1):\n",
    "            idx = backpointers[best_id, i]\n",
    "            predtags.append(tags[idx])\n",
    "            best_id = idx\n",
    "\n",
    "        #return reversed predtags\n",
    "        return predtags[::-1]         \n",
    "\n",
    "    \n",
    "    def predict_greedy(self, words):\n",
    "        '''\n",
    "        greedy prediction\n",
    "        '''\n",
    "        context = [self.START] + [self.normalize(word) for word in words] + [self.END, self.END]\n",
    "                \n",
    "        prev_predicted_tag = self.START\n",
    "        prev_predicted_tag2 = self.START\n",
    "\n",
    "        out = []\n",
    "\n",
    "        for j, word in enumerate(words):\n",
    "            # for unambiguous words, just look up the tag\n",
    "            if len(self.tag_dict[context[j+1]]) == 1:\n",
    "                predicted_tag = list(self.tag_dict[context[j+1]])[0]\n",
    "            else:\n",
    "                predicted_tag = None\n",
    "\n",
    "            if not predicted_tag:\n",
    "                # get the predicted features. NB: use j+1, since context is longer than words\n",
    "                prediction_features = self.get_features(word, prev_predicted_tag2, prev_predicted_tag, context, j+1)\n",
    "                scores = self.get_scores(prediction_features)\n",
    "                \n",
    "                # predict the current tag\n",
    "                predicted_tag = max(scores, key=scores.get)\n",
    "\n",
    "            prev_predicted_tag2 = prev_predicted_tag\n",
    "            prev_predicted_tag = predicted_tag\n",
    "\n",
    "            out.append(predicted_tag)\n",
    "\n",
    "        return out\n",
    "    \n",
    "        \n",
    "    def read_conll_file(self, file_name):\n",
    "        \"\"\"\n",
    "        read in a file with CoNLL format:\n",
    "        word1    tag1\n",
    "        word2    tag2\n",
    "        ...      ...\n",
    "        wordN    tagN\n",
    "\n",
    "        Sentences MUST be separated by newlines!\n",
    "        \"\"\"\n",
    "        current_words = []\n",
    "        current_tags = []\n",
    "\n",
    "        with open(file_name, encoding='utf-8') as conll:\n",
    "            for line in conll:\n",
    "                line = line.strip()\n",
    "\n",
    "                if line:\n",
    "                    word, tag = line.split('\\t')\n",
    "                    current_words.append(word)\n",
    "                    current_tags.append(tag)\n",
    "\n",
    "                else:\n",
    "                    yield (current_words, current_tags)\n",
    "                    current_words = []\n",
    "                    current_tags = []\n",
    "\n",
    "        # if file does not end in newline (it should...), check whether there is an instance in the buffer\n",
    "        if current_tags != []:\n",
    "            yield (current_words, current_tags)\n",
    "        \n",
    "\n",
    "    def save(self, file_name):\n",
    "        \"\"\"\n",
    "        save model as pickle file\n",
    "        \"\"\"\n",
    "        print(\"saving model...\", end=' ', file=sys.stderr)\n",
    "        with open(file_name, \"wb\") as model:\n",
    "            # pickle cannot save default_dictionaries\n",
    "            # => make copy and turn into regular dictionaries\n",
    "            save_feature_weights = defaultdict(lambda: defaultdict(float))\n",
    "            save_feature_weights.update(self.feature_weights)\n",
    "            save_tag_dict = defaultdict(set)\n",
    "            save_tag_dict.update(self.tag_dict)\n",
    "\n",
    "            save_feature_weights.default_factory = None\n",
    "            save_tag_dict.default_factory = None\n",
    "            pickle.dump((save_feature_weights, save_tag_dict, self.tags),\n",
    "                     model, -1)\n",
    "        print(\"done\", file=sys.stderr)\n",
    "\n",
    "\n",
    "    def load(self, file_name):\n",
    "        \"\"\"\n",
    "        load model from pickle file\n",
    "        \"\"\"\n",
    "        print(\"loading model...\", end=' ', file=sys.stderr)\n",
    "        with open(file_name, 'rb') as model:\n",
    "            try:\n",
    "                parameters = pickle.load(model)\n",
    "            except IOError:\n",
    "                msg = (\"No such model file.\")\n",
    "                raise MissingCorpusError(msg)\n",
    "\n",
    "            feature_weights, tag_dict, tags = parameters\n",
    "            self.tags = tags\n",
    "\n",
    "            # pickle cannot store defaultdicts, so we need a 2-step process\n",
    "            # 1. initialize\n",
    "            self.feature_weights = defaultdict(lambda: defaultdict(float))\n",
    "            self.tag_dict = defaultdict(set)\n",
    "            \n",
    "            # 2. update\n",
    "            self.feature_weights.update(feature_weights)\n",
    "            self.tag_dict.update(tag_dict)\n",
    "        print(\"done\", file=sys.stderr)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9353472",
   "metadata": {},
   "source": [
    "To train and save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff731f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = StructuredPerceptron()\n",
    "inference_method = 'greedy'\n",
    "\n",
    "train_file = 'path/to/some_file.data'\n",
    "dev_file = 'path/to/some_other_file.data'\n",
    "\n",
    "# train model on data\n",
    "sp.fit(train_file, dev_file=dev_file, iterations=10, inference=inference_method)\n",
    "\n",
    "# save model to disk\n",
    "sp.save('model_greedy.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23723ec3",
   "metadata": {},
   "source": [
    "To load and run a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9f45ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generic SP instance\n",
    "sp = StructuredPerceptron()\n",
    "\n",
    "# load parameters of trained model\n",
    "sp.load('model_greedy.pickle')\n",
    "\n",
    "# to use it on sentences\n",
    "print(sp.predict('Their management plan reforms worked'.split(), method='greedy'))\n",
    "\n",
    "print(sp.predict(\"Attack was their best option\".split(), method='greedy'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
